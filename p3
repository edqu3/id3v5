P3.

discriminant function
comes from LDA. LDA assumes the p(x | y) are normally dist.
N (mean, covariance)

in 1 NN case, we define the class assignments by distance.
so P(x|w1) = .5, P(x|w2) = .5 turn




when the probability distribution is normal, LDA is very similar to logistic 

regression


last step of lda
use bayes theorem to flip the p(x|Y)





There are only 2 classes, w_1, w_2

were only considering the nearest neighbor.
1 NN, the class assigned comes from the nearest neighbor.
get distance(x, y) from eucleadian distance

then if we had an instance x with 2 attributes: x = {1,3}

P(x|w1) = the prob of feature vector x is classified as w1

if x = {1,3} and x falls between 0 and 1.
2*{1,3} = p(x|w)

is my bias w_o = .5 ?

g(x) = W^t * X + w_0 is our discriminant function
we must define a threshold for our discriminant function g(x)
such that g(x) > c => w1 and w2 othewise

p(x|w1) = p(w1|x)*p(x) / p(w1)

p(x|w2) = p(w2|x)*p(x) / p(w2)

in the case of 1NN w^t is the distance between our x and y from the 

distribution.


NOTE: the decision boundary is exactly in the middle if each class is 

equally likely, THIS IS EXACTLY OUR CASE. 

2x = 2(1-x)
2x = 2 - 2x
4x = 2
x  = 0.5	intersection

g(x) = 0 defines the decision surafce that separates classes.

Visual representation of data
for a 2 dimensional feature vector with target class w

X = Attr 1, Attr 2

Attr 1		Attr 2 		Target
x1		x2		w1
x3		x4		w1
.
.
.
x11		x12		w2
x13		x14		w2


P(X|w1) = P(X,W1) / P(w1)

d(X) = eucledian distance = some number 

i / .5 

so for a feature vector [ 2 , 6 ]

sqrt[(2 - x1)^2 + (6 - x2)^2] = sqrt[ 17 ] = 4.12

noo.....

http://www.byclb.com/TR/Tutorials/neural_networks/ch9_1.htm





